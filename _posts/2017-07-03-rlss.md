---
layout: post
title: Reinforcement Learning Summer School
description: Some thoughts after attending RLSS at the Montreal Institute for Learning Algorithms.
comments: true
published: true
feature: assets/img/rlss/deep_dream.png
---

### Reinforcement learning: the most promising path to artificial general intelligence.

Someday we might be able to have real conversations with robots. We might be able to show them how to do the dishes, and maybe trust them not to knock over our houseplants. If that day comes, the robots' intelligence will probably be powered by [*reinforcement learning*](https://en.wikipedia.org/wiki/Reinforcement_learning). What is it, and how does it work? I learned some of the basics last week, when I attended the [Reinforcement Learning Summer School](https://mila.umontreal.ca/en/cours/deep-learning-summer-school-2017/) at the [Montreal Institute for Learning Algorithms](https://mila.umontreal.ca/en/). In this blog post, I explain some key lessons and takeaways from the week.

### Joelle Pineau: what is reinforcement learning?

[Joelle Pineau](http://www.cs.mcgill.ca/~jpineau/) kicked things off with a wonderful introductory lecture, which I've drawn from heavily in these first few sections of my blog post.

Let's enumerate a few of the tasks that you perform effortlessly as you go about your day:

1. You perceive things in your environment. (Say, you see a bus.)
2. You form beliefs about your perceptions. (You come to believe that the bus is being driven by a professional driver who intends to do her job properly today.)
3. You take actions that have effects on your environment. (You run to the bus stop, causing the bus to slow down so you can board.)
4. You plan future actions in hopes of achieving one or more of your goals. (You plan to get off the bus after five stops, because then you'll be within walking distance of work.)

These tasks are the core of what constitutes an intelligent agent, i.e. a goal-directed system that interacts with an environment. And that's what reinforcement learning attempts to codify. The mathematical structure at its heart is called a [*Markov Decision Process*](https://en.wikipedia.org/wiki/Markov_decision_process), or MDP. Let's break that down.

### What is a Markov Chain?

MDPs are an extension of what are called [Markov Chains](https://en.wikipedia.org/wiki/Markov_chain), which sounds scary but isn't. Say you're scrolling through Twitter. You see a link and click on it, which takes you to a webpage. While reading the webpage, maybe you click another link which brings you back to Twitter. We can give a basic description of your chain of actions with two pieces of information: (1) your *state*, i.e. what page you're currently looking at; and (2) your *transition probability*, i.e. the chance that you'll move from your current state to some other state. A Markov Chain is just this – a delineation of all possible states, and of all probabilities of transitioning from each state to any other state. If we can understand the mathematical structure of a Markov Chain, we can make decent predictions about which sequences of events are likely to occur, and which states we're likely to find ourselves in if the chain runs for a long time. For a really nice visual explanation of Markov Chains, see [here](http://setosa.io/ev/markov-chains/).

### What is a Markov Decision Process (MDP)?

Now, Markov Chains don't include any notion of decision-making, or of an agent that drives the transitions from one state to another. So if we want a model of an intelligent agent, we'll need something more. That's what an MDP gives us.

1. Like in a Markov Chain, there is a set of states, *S*.
2. But instead of there being a set of transition probabilities that link each state with every other state, there is a set of possible actions that an agent might take, *A*. All actions simply attempt to move the agent from one state to another.
3. Of course, real-world environments often have an element of uncertainty; we sometimes take one action, but we end up in an unintended state. So an MDP also defines a model of the dynamics of the environment. The dynamics model is a set of transition probabilities that are conditioned on the current state (as in a Markov Chain) *and* on the action: *T(s,a,s') = Pr(s<sub>t+1</sub>*\|*s<sub>t</sub>,a<sub>t</sub>)*. Here, *T* is the transition probability, *s* (or *s<sub>t</sub>*) is the current state, *a* is the action, and *s'* (or *s<sub>t+1</sub>*) is the future state.
4. If we want our agent to be able to learn to perform certain actions rather than flail randomly, we need some notion of a *reward*. This is usually denoted *R*, and it is a scalar number that is associated with each state.
5. Lastly, we need to know the initial state distribution, i.e. where the agent is likely to find itself before it takes any actions.

So, given an MDP, how do we end up with intelligent behaviour? We set the agent loose in the environment, allow it to flail, and then reset it to an initial state – again and again and again, each time giving slightly more preference to the actions that maximize the sum of the rewards. The ultimate goal is to learn the optimal *policy*. A policy is simply a one-to-one mapping of actions to states: "given that I find myself in this state, I will take that action." The *optimal* policy is the one that maximizes the sum of the rewards, or the *return*. Once the optimal policy has been found, the learning process is complete, and the MDP will now behave exactly like a Markov Chain!

### But intelligence is more complicated than that...

AI researchers have known about MDPs for a long time; [Richard Bellman](https://en.wikipedia.org/wiki/Richard_E._Bellman) first described them in a [1957 paper](http://www.iumj.indiana.edu/IUMJ/FULLTEXT/1957/6/56038). Knowing this, you probably recognize that understanding MDPs doesn't get you a robot that can learn to do the dishes. That's because MDPs are extreme simplifications of the real world. For one thing, it would be clearly impossible to describe every possible state of the world, because there are infinite states. Similarly, there are infinite actions that an agent might take, and those actions are usually continuous rather than episodic. Real-world agents also don't get to be reset to an initial state hundreds of times before they learn the right actions.

So, think of it this way: MDPs are to reinforcement learning as a springboard is to the ocean. Much much much more has been done. I know barely any of it, but I did come away from RLSS with a few takeaways for anyone who is interested in the current state of AI research.

### Rich Sutton: The future of AI is in methods that scale with computation.

[Rich Sutton](https://en.wikipedia.org/wiki/Richard_S._Sutton), one of the founding fathers of reinforcement learning, gave a fascinating talk about [temporal difference (TD) learning](https://en.wikipedia.org/wiki/Temporal_difference_learning). Basically, TD learning attempts to solve the problem of needing an agent to learn moment-by-moment, rather than by updating its policies only after observing a complete sequence of actions. For example, if you're playing chess, you shouldn't need to play the entire game to know whether you're making good moves. Similarly, if you want to teach your robot how to do the dishes, you won't have the luxury of letting it flail its way through a hundred thousand iterations of the Wash Dishes game while it tries to get it right. You need to be able to give it instant feedback. This is important (a) because we won't have the time or resources to let an agent learn via a hundred thousand failures, (b) because the real world has innumerable states whose values can't *all* be ascertained, and (c) because we won't always be able to rely on human labelling of desirable states.

### Hado van Hasselt: Reinforcement learning + deep learning = the coming AI revolution.

[Hado van Hasselt](https://hadovanhasselt.com/), a researcher at DeepMind, gave a talk about deep reinforcement learning – an approach that combines the standard tools of reinforcement learning with the deep neural networks that have had so much success in supervised learning in recent years. The industrial revolution changed the world by automating known solutions to repetitive manual problems; the digital revolution changed the world by automating known solutions to repetitive informational problems; the AI revolution will change the world by automating the very task of solving problems. Hasselt hypothesizes that when the AI revolution comes into full force, reinforcement learning will provide the framework for decision-making, and deep learning will provide the tools to learn the components of the optimal models.

### Nando de Freitas: Physics and embodiment are essential for understanding intelligence.

[Nando de Freitas](http://www.cs.ubc.ca/~nando/), an Oxford/UBC/Deepmind researcher, gave a talk about deep control, the problem of designing intelligent systems that can have predictable and desirable interactions with the physical world. On first blush, it might seem that designing a robot that can walk or roll or grasp an object would be one of the easier problems in AI research. After all, it's just inverse kinematics, right? Well, even still, the physical world is a heck of a lot more complicated than the 2D world of PacMan, which is still an active research area in RL. And many of the real world's physical properties may turn out to interact with artificial learning in unexpected ways. For example, most human adults know how to walk. But most of them learned this task as infants, when the cost of falling was very low. If we had to learn to walk as adults, with constant falls from an adult height, we might well die. De Freitas also spoke about the challenges that researchers will face when developing methods that will allow AIs to understand instructions given to it in natural language, with all its ambiguities. The gist is that it will be *hard*. De Freitas's talk was one of my favourites, and it drove home just how far we are from creating true artificial general intelligence.

### Csaba Szepesvari: Empiricism in AI research is not enough.

[Csaba Szepesvari](https://sites.ualberta.ca/~szepesva/index.html), a researcher at the University of Alberta, gave a talk about the *theory* of reinforcement learning, and of machine learning more generally. This is something that I don't know much about, but the talk inspired me to learn more. Luckily, Szepesvari wrote [a book about it](https://sites.ualberta.ca/~szepesva/RLBook.html), and it's freely accessible on his website. The main takeaway from the talk was that we won't be able to make significant advances in AI research unless we can understand and produce provable theorems about statistical systems.

### Philip S. Thomas: A safe algorithm is one that is guaranteed, with some probability, to not make things worse.

[Philip Thomas](http://psthomas.com/), a researcher at Carnegie Mellon, gave a very interesting and refreshing talk about AI safety. The concerns that I often hear voiced about AI safety are about existential risks. I think these concerns are valid, but lately I've begun to worry that too few people are sufficiently worried about *non*-existential risks from AI. I mean, AI doesn't have to be vastly more competent than its creators for it to cause harm. And it will be seriously non-trivial to make sure our reinforcement learning algorithms are safe even in the most basic sense. Thomas gave a good demonstration of where we're at right now: he asked the members of the audience to raise their hands if they had ever implemented an RL algorithm to try to solve a real-world problem; about half (100 of 200 people) raised their hands; he then asked them to keep their hands up if this process had required the tuning of hyperparameters; everybody kept their hands up; he then asked them to keep their hands up if they'd ever gotten those tunings right on their first try; everybody lowered their hands. If reinforcement learning will truly make its mark on the world, AI researchers will have to figure out how to guarantee that their algorithms will, at minimum, not make things worse than the existing solution. This won't be easy, but serious work is already underway, which is good.

### What now?

For now I don't have much cause to apply reinforcement learning in my work, so I don't anticipate diving too much deeper into it while I have other skills to develop. But I'm 100% convinced that RL will be critical for the future of AI, and it's extremely interesting. I might even go back to school to study it sometime in the next few years. If I do, I know which resources to consult:

* [github.com/aikorea/awesome-rl](https://github.com/aikorea/awesome-rl). A github repository with its own list of resources.
* [Rich Sutton's textbook](http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf). I've read parts, and they're totally lucid, just like Sutton's talk was. He's the real deal.
* [The Udacity course on RL](https://classroom.udacity.com/courses/ud600). I've watched some of the videos, and they're pretty good.
* [Csaba Szepeszvari's textbook](https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf). I've only skimmed, and it's dense but good.
* [github.com/pierrelux/rlss2017](https://github.com/pierrelux/rlss2017). A handy github repository with some Jupyter notebooks, put together specifically for RLSS.

If any of this sounds exciting to you, I highly recommend applying to RLSS next year! 
